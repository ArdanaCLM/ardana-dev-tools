#!/bin/bash
#
# (c) Copyright 2019 SUSE LLC
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
# Run through the Ardana update workflow to update the Cloud
# within the same stream using updated package sources.
#

set -eux
set -o pipefail

cmd_name="$(basename ${BASH_SOURCE[0]})"
cmd_dir="$(dirname "$(readlink -e "${BASH_SOURCE[0]}")")"
adt_top="$(dirname "${cmd_dir}")"
ansible_dir="${adt_top}/ansible"
log_file="${adt_top}/logs/${cmd_name}.log"

eval "$(${cmd_dir}/ardana-env)"

. ${cmd_dir}/lib/service_groups.sh
. ${cmd_dir}/lib/deployer_utils.sh

usage() {
    set +x
    echo "${cmd_name} [options] [cloud]"
    echo
    echo "cloud defaults to adt, a minimal 2 node cloud"
    echo
    echo "--all-services      -- Enable all services in initial Cloud deployment."
    echo "--adv-services      -- Enable advanced services in initial Cloud deployment."
    echo "--basic-services    -- Enable basic services in initial Cloud deployment."
    echo "--min-services      -- Enable minimal services in initial Cloud deployment."
    echo "--no-mml-services   -- Disable all related MML services in Cloud deployment."
    echo "--c8, --c8-devel    -- Test DC8 ==> DC8S update."
    echo "--c8-hos            -- Use the HOS flavour of Cloud8; SOC is default."
    echo "--c8-pool           -- Test C8 Pool ==> Updates update."
    echo "--c8-updates        -- Test C8 Pool + Updates ==> Updates-test update."
    echo "--c9, --c9-devel    -- Test DC9 ==> DC9S update. (default)"
    echo "--c9-pool           -- Test C9 Pool ==> Updates update."
    echo "--c9-updates        -- Test C9 Pool + Updates ==> Updates-test update."
    echo "--enable            -- Specify service-component to enable. (repeatable)"
    echo "--disable           -- Specify service-component to disable. (repeatable)"
    echo "--no-setup          -- Don't run dev-env-install.yml."
    echo "--no-update-rpms    -- Don't build RPMs from local clones."
    echo "--patch             -- Use the zypper patch method."
    echo "--pre-destroy       -- Destroy any existing instance of cloud first."
    echo "--rhel-compute      -- Enable RHEL computes."
    echo "--sles-compute      -- Enable SLES computes (default)."
    echo "--update            -- Use the zypper update method."
}

# Manage list of long options as a sorted array of option names which
# we then join with commas to form the argument to getopt
long_opts=(
    all-services
    adv-services
    basic-services
    c8
    c8-devel
    c8-hos
    c8-pool
    c8-updates
    c9
    c9-devel
    c9-pool
    c9-updates
    ci
    help
    min-services
    no-mml-services
    no-setup
    no-update-rpms
    patch
    pre-destroy
    rhel-compute
    sles-compute
    update
)

# join long_opts members with ","
printf -v OPTIONS ",%s" "${long_opts[@]:1}"
OPTIONS="${long_opts[0]}${OPTIONS}"

TEMP=$(getopt -o -h -l $OPTIONS -n $cmd_name -- "${@}")
if (( $? != 0 ))
then
    echo "Terminating..." >&2
    exit 1
fi

# Note the quotes around `$TEMP': they are essential!
eval set -- "$TEMP"

cloud_version=
pre_destroy=
zypper_method=
common_args=()
cloud_args=( --no-update-rpms )  # only build RPMs for update cloud
update_args=()
svc_groups_to_enable=()
svc_groups_to_disable=()

while true ; do
    case "${1}" in
    (-h|--help)
        usage
        exit 0
        ;;
    (--no-*-services)
        svc_group="${1#--no-}"
        svc_group="${svc_group%-services}"
        svc_groups_to_disable+=( "${svc_group}" )
        shift
        ;;
    (--*-services)
        svc_group="${1#--}"
        svc_group="${svc_group%-services}"
        svc_groups_to_enable+=( "${svc_group}" )
        shift
        ;;
    (--enable)
        set_service_state "${2}" true
        shift 2
        ;;
    (--disable)
        set_service_state "${2}" false
        shift 2
        ;;
    (--c8|--c8-*|--c9|--c9-*)
        cloud_version="${1:3:1}"
        case "${1}" in
        (*-devel)
            cloud_args+=( "--c${cloud_version}-devel" )
            update_args+=( "--c${cloud_version}-staging" )
            ;;
        (*-hos)
            if [[ "${cloud_version}" != "8" ]]
            then
                echo "ERROR: There is no HOS variant for Cloud${cloud_version}!"
                exit 1
            fi
            common_args+=( "${1}" )
            ;&
        (*-pool)
            cloud_args+=( "--c${cloud_version}-pool" )
            update_args+=( "--c${cloud_version}-updates" )
            ;;
        (*-updates)
            cloud_args+=( "--c${cloud_version}-updates" )
            update_args+=( "--c${cloud_version}-updates-test" )
            ;;
        esac
        shift
        ;;
    (--rhel-compute|--sles-compute)
        common_args+=( "${1}" )
        shift
        ;;
    (--ci|--no-setup|--no-update-rpms)
        common_args+=( "${1}" )
        shift
        ;;
    (--patch|--update)
        zypper_method="${1:2}"
        shift
        ;;
    (--pre-destroy)
        pre_destroy="${1}"
        shift
        ;;
    (--)
        shift
        break
        ;;
    (*)
        break
        ;;
    esac
done

if [[ -z "${cloud_version}" ]]
then
    cloud_version=9
fi

if [[ -z "${zypper_method}" ]]
then
    zypper_method=update
fi

# Ensure common args has something in it to keep bash checking happy
common_args+=( --c${cloud_version} )

if (( ${#cloud_args[@]} == 0 ))
then
    cloud_args+=( "--c${cloud_version}-devel" )
fi

if (( ${#update_args[@]} == 0 ))
then
    update_args+=( "--c${cloud_version}-staging" )
fi

# Setup default service group enablement/disablement
if (( ${#svc_groups_to_enable[@]} == 0 ))
then
    svc_groups_to_enable+=( all )
fi
#if (( ${#svc_groups_to_disable[@]} == 0 ))
#then
#    # disable until MML stack services are upgradable
#    svc_groups_to_disable+=( mml )
#fi

# Determine cloud_name and associated vagrant_dir
cloud_name="${1:-adt}"
vagrant_dir="${adt_top}/ardana-vagrant-models/${cloud_name}-vagrant"

# Setup specified services for initial cloud deployment
setup_services_groups "${cloud_version}" svc_groups_to_enable svc_groups_to_disable

# Add --disabled-services entries to cloud_args
generate_disabled_services_entries cloud_args

# Initialise the deployer utils environment BEFORE any output direction
setup_deployer_utils "${cloud_name}"

# log stdout & stderr to file as well as terminal
# NOTE: may need to use less -R to read log content if running against a terminal
exec > >(tee "${log_file}")
exec 2>&1

echo "Updating Cloud${cloud_version} model '${cloud_name}' using zypper '${zypper_method}'"

set -x

# Retrieve artifacts for initial cloud and updates runs
${cmd_dir}/astack.sh \
    "${common_args[@]}" \
    "${cloud_args[@]}" \
    ${pre_destroy} \
    --no-cloud \
    "${cloud_name}"

find "${adt_top}/../C${cloud_version}_NEW_RPMS" -type f -name "*.rpm"

# no need to run setup phase again, so skip in future astack invocations
common_args+=( --no-setup )

${cmd_dir}/astack.sh \
    "${common_args[@]}" \
    "${update_args[@]}" \
    --no-update-rpms \
    --no-cloud \
    "${cloud_name}"

find "${adt_top}/../C${cloud_version}_NEW_RPMS" -type f -name "*.rpm"

# all artifacts have been retrieved so no need to retrieve again
common_args+=( --no-artifacts )

# Force re-creation of the local input-model cache
rm -rf "${vagrant_dir}/input-model"

# Bring up the cloud, using already fetched artifacts, running
# tempest tests to confirm correct operation.
${cmd_dir}/astack.sh \
    "${common_args[@]}" \
    "${cloud_args[@]}" \
    --run-tests \
    "${cloud_name}"

find "${adt_top}/../C${cloud_version}_NEW_RPMS" -type f -name "*.rpm"

source ${cmd_dir}/libci.sh

pushd "${ansible_dir}"

# disable/remove existing zypper repos
${cmd_dir}/ardana --cloud ${cloud_name} \
    ansible-playbook \
        -i hosts/cloud.yml \
        ${ansible_dir}/cloud-repos-update.yml \
        -e zypper_repo_enabled=no \
        -e zypper_repo_state=absent

# Switch to update cloud settings
${cmd_dir}/astack.sh \
    "${common_args[@]}" \
    "${update_args[@]}" \
    --no-cloud \
    "${cloud_name}"

# create/enable new zypper repos
${cmd_dir}/ardana --cloud ${cloud_name} \
    ansible-playbook \
        -i hosts/cloud.yml \
        cloud-repos-update.yml

# Install/Update Cloud pattern packages and run ardana-init, but skip model init
# TODO(fergal): Convert to running ardana-update-pkgs
${cmd_dir}/ardana --cloud ${cloud_name} \
    ansible-playbook \
        -i hosts/cloud.yml \
        cloud-deployer-init.yml \
        -e skip_model_init

# Run config-cloud.sh utility script to regenerate the config processor data
# and recreate the scratch area.
deployer_run ${cmd_dir}/deployer/config-cloud.sh

# run the update-pkgs playbook
ardana_scratch_run_playbook \
    OPS-LM--first-member:tempest_all \
    ardana-update-pkgs \
    -e zypper_update_method=${zypper_method} \
    -e zypper_update_gpg_checks=true \
    -e zypper_update_licenses_agree=true \
    -e zypper_update_include_reboot_patches=true

if deployer_needs_update
then
    ardana_scratch_run_playbook \
        OPS-LM--first-member:tempest_all \
        ardana-update
fi

if deployer_needs_reboot
then
    # this will fail saying deployer node needs to be manually rebooted
    # and that we need to run _ardana-post-reboot.yml after rebooting.
    ardana_scratch_run_playbook \
        OPS-LM--first-member \
        ardana-reboot || true

    # Now "manually" reboot the deployer node
    ${cmd_dir}/ardana --cloud ${cloud_name} \
        ansible-playbook \
            -i hosts/cloud.yml \
            ${ansible_dir}/cloud-deployer-reboot.yml

    # now run the _ardana_post-reboot.yml playbook
    ardana_scratch_run_playbook \
        OPS-LM--first-member \
        _ardana-post-reboot
fi

# use update-non-deployer-nodes to update the remaining nodes in the cloud
deployer_run ${cmd_dir}/deployer/update-non-deployer-nodes update

# use run-tests.sh to validate cloud is working correctly
deployer_run ${cmd_dir}/deployer/run-tests.sh ci ${cloud_name}

# vim:shiftwidth=4:tabstop=4:expandtab
